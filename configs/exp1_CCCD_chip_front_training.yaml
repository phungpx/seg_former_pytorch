data:
  train:
    module: torch.utils.data
    class: DataLoader
    DataLoader:
      dataset:
        module: flame.core.data.field_dataset
        class: FieldDataset
        FieldDataset:
          dirnames:
            - '''/extdata/ocr/phungpx/projects/PHUNGPX/semantic_segmentation_pytorch/dataset/CCCD_front_chip/train/CCCD_front_chip_NORMAL'''
            - '''/extdata/ocr/phungpx/projects/PHUNGPX/semantic_segmentation_pytorch/dataset/CCCD_front_chip/train/CCCD_front_chip_V_BP1'''
            - '''/extdata/ocr/phungpx/projects/PHUNGPX/semantic_segmentation_pytorch/dataset/CCCD_front_chip/train/CCCD_front_chip_V_NAME2_4_WORDS'''
            - '''/extdata/ocr/phungpx/projects/PHUNGPX/semantic_segmentation_pytorch/dataset/CCCD_front_chip/train/CCCD_front_chip_V_NAME2_5_WORDS'''
          classes:
            BACKGROUND: [[0, 0, 0], 0, False, False]  # color, class_id, reduce_height, reduce_width
            HEADING: [[175, 153, 144], 1, True, False]
            HEADING_VI: [[175, 153, 144], 1, True, False]
            V_ID: [[75, 25, 230], 2, True, False]
            # V_NAME1: [[128, 0, 0], 3, True, False]
            V_NAME2: [[48, 130, 245], 3, True, False]
            V_BD: [[128, 128, 0], 4, True, False]
            V_SEX: [[0, 101, 255], 5, True, False]
            V_NAT: [[100, 150, 255], 6, True, False]
            V_BP1: [[25, 225, 225], 7, True, False]
            V_BP2: [[75, 180, 60], 8, True, False]
            V_A1: [[180, 215, 255], 9, True, False]
            V_A2: [[240, 240, 70], 10, True, False]
            V_TL: [[100, 0, 100], 11, True, False]
            FIGURE: [[255, 190, 230], 12, False, False]
            QR_CODE: [[70, 70, 70], 13, False, False]
            QR: [[70, 70, 70], 13, False, False]
          reduce_ratios: (0.25, 0.25)
          image_size: (512, 512)
          image_extents: ['''.jpg''', '''.png''', '''.jpeg''', '''.JPG''', '''.PNG''', '''.JPEG''']
          label_extent: '''.json'''
          transforms:
            - iaa.Add(value=(-100, 100), per_channel=True)
            - iaa.GaussianBlur(sigma=(0, 1))
            - iaa.MotionBlur()
            - iaa.Affine(rotate=(0, 360), shear=(-20, 20), fit_output=True)
            - iaa.PerspectiveTransform(scale=(0, 0.1))
            - iaa.Crop(percent=(0, 0.1))
            - iaa.Pad(percent=(0, 0.1))
            - iaa.JpegCompression(compression=(0, 30))
            - iaa.Rot90(k=[0, 1, 2, 3], keep_size=False)
            - iaa.Fliplr(p=0.5)
            - iaa.Flipud(p=0.5)
            - iaa.Grayscale(alpha=(0.0, 1.0))
            - iaa.ChangeColorTemperature()
            - iaa.Clouds()
            - iaa.Dropout()
      batch_size: 16
      shuffle: True
      pin_memory: True
      num_workers: 12
      drop_last: False

  train_eval:
    module: torch.utils.data
    class: DataLoader
    DataLoader:
      dataset:
        module: flame.core.data.field_dataset
        class: FieldDataset
        FieldDataset:
          dirnames:
            - '''/extdata/ocr/phungpx/projects/PHUNGPX/semantic_segmentation_pytorch/dataset/CCCD_front_chip/train/CCCD_front_chip_NORMAL'''
            - '''/extdata/ocr/phungpx/projects/PHUNGPX/semantic_segmentation_pytorch/dataset/CCCD_front_chip/train/CCCD_front_chip_V_BP1'''
            - '''/extdata/ocr/phungpx/projects/PHUNGPX/semantic_segmentation_pytorch/dataset/CCCD_front_chip/train/CCCD_front_chip_V_NAME2_4_WORDS'''
            - '''/extdata/ocr/phungpx/projects/PHUNGPX/semantic_segmentation_pytorch/dataset/CCCD_front_chip/train/CCCD_front_chip_V_NAME2_5_WORDS'''
          classes:
            BACKGROUND: [[0, 0, 0], 0, False, False]  # color, class_id, reduce_height, reduce_width
            HEADING: [[175, 153, 144], 1, True, False]  # heading
            HEADING_VI: [[175, 153, 144], 1, True, False]  # heading
            V_ID: [[75, 25, 230], 2, True, False]
            # V_NAME1: [[128, 0, 0], 3, True, False]
            V_NAME2: [[48, 130, 245], 3, True, False]
            V_BD: [[128, 128, 0], 4, True, False]
            V_SEX: [[0, 101, 255], 5, True, False]
            V_NAT: [[100, 150, 255], 6, True, False]
            V_BP1: [[25, 225, 225], 7, True, False]
            V_BP2: [[75, 180, 60], 8, True, False]
            V_A1: [[180, 215, 255], 9, True, False]
            V_A2: [[240, 240, 70], 10, True, False]
            V_TL: [[100, 0, 100], 11, True, False]
            FIGURE: [[255, 190, 230], 12, False, False]
            QR_CODE: [[70, 70, 70], 13, False, False]  # qrcode
            QR: [[70, 70, 70], 13, False, False]  # qrcode
          reduce_ratios: (0.25, 0.25)
          image_size: (512, 512)
          image_extents: ['''.jpg''', '''.png''', '''.jpeg''', '''.JPG''', '''.PNG''', '''.JPEG''']
          label_extent: '''.json'''
      batch_size: 16
      shuffle: False
      pin_memory: True
      num_workers: 12
      drop_last: False

  valid:
    module: torch.utils.data
    class: DataLoader
    DataLoader:
      dataset:
        module: flame.core.data.field_dataset
        class: FieldDataset
        FieldDataset:
          dirnames:
            - '''/extdata/ocr/phungpx/projects/PHUNGPX/semantic_segmentation_pytorch/dataset/CCCD_front_chip/valid/CCCD_front_chip_NORMAL'''
            - '''/extdata/ocr/phungpx/projects/PHUNGPX/semantic_segmentation_pytorch/dataset/CCCD_front_chip/valid/CCCD_front_chip_V_BP1'''
            - '''/extdata/ocr/phungpx/projects/PHUNGPX/semantic_segmentation_pytorch/dataset/CCCD_front_chip/valid/CCCD_front_chip_V_NAME2_4_WORDS'''
            - '''/extdata/ocr/phungpx/projects/PHUNGPX/semantic_segmentation_pytorch/dataset/CCCD_front_chip/valid/CCCD_front_chip_V_NAME2_5_WORDS'''
          classes:
            BACKGROUND: [[0, 0, 0], 0, False, False]  # color, class_id, reduce_height, reduce_width
            HEADING: [[175, 153, 144], 1, True, False]
            HEADING_VI: [[175, 153, 144], 1, True, False]
            V_ID: [[75, 25, 230], 2, True, False]
            # V_NAME1: [[128, 0, 0], 3, True, False]
            V_NAME2: [[48, 130, 245], 3, True, False]
            V_BD: [[128, 128, 0], 4, True, False]
            V_SEX: [[0, 101, 255], 5, True, False]
            V_NAT: [[100, 150, 255], 6, True, False]
            V_BP1: [[25, 225, 225], 7, True, False]
            V_BP2: [[75, 180, 60], 8, True, False]
            V_A1: [[180, 215, 255], 9, True, False]
            V_A2: [[240, 240, 70], 10, True, False]
            V_TL: [[100, 0, 100], 11, True, False]
            FIGURE: [[255, 190, 230], 12, False, False]
            QR_CODE: [[70, 70, 70], 13, False, False]
            QR: [[70, 70, 70], 13, False, False]
          reduce_ratios: (0.25, 0.25)
          image_size: (512, 512)
          image_extents: ['''.jpg''', '''.png''', '''.jpeg''', '''.JPG''', '''.PNG''', '''.JPEG''']
          label_extent: '''.json'''
      batch_size: 16
      shuffle: False
      pin_memory: True
      num_workers: 12
      drop_last: False

loss:
  module: flame.core.loss.OHEM_cross_entropy
  class: OHEMCrossEntropy
  OHEMCrossEntropy:
    ignore_label: 255
    weight: None
    thresh: 0.7
    output_transform: 'lambda x: (x[0], x[1])'

model:
  module: flame.core.model.segformer
  class: Model
  Model:
    backbone_pretrained: '''checkpoint/seg_former_pretrained/backbone_pretrained_weights/mit_b0.pth'''
    pretrained: null
    version: '''B0'''
    num_classes: 14

optim:
  module: torch.optim
  class: AdamW
  AdamW:
    params: config['model'].parameters()
    lr: 0.001
    weight_decay: 0.01

train_evaluator:
  module: flame.handlers.metric_evaluator
  class: MetricEvaluator
  MetricEvaluator:
    dataset: config['data']['train_eval']
    device: '''cuda'''

valid_evaluator:
  module: flame.handlers.metric_evaluator
  class: MetricEvaluator
  MetricEvaluator:
    dataset: config['data']['valid']
    device: '''cuda'''

metrics:
  module: flame.handlers.metric_evaluator
  class: Metrics
  Metrics:
    metrics:
      OHEM_ce_loss:
        module: flame.core.metric.loss
        class: Loss
        Loss:
          loss_fn: config['loss']
          output_transform: 'lambda x: (x[0], x[1])'
      Dices:
        module: ignite.metrics
        class: DiceCoefficient
        DiceCoefficient:
          cm:
            module: ignite.metrics.confusion_matrix
            class: ConfusionMatrix
            ConfusionMatrix:
              num_classes: 14
              average: None
              output_transform: 'lambda x: (x[0], x[1])'
              device: '''cuda'''
          ignore_index: 0  # background
      IoU:
        module: ignite.metrics
        class: IoU
        IoU:
          cm:
            module: ignite.metrics.confusion_matrix
            class: ConfusionMatrix
            ConfusionMatrix:
              num_classes: 14
              average: None
              output_transform: 'lambda x: (x[0], x[1])'
              device: '''cuda'''
          ignore_index: 0  # background
      mIoU:
        module: ignite.metrics.metrics_lambda
        class: MetricsLambda
        MetricsLambda:
          f: 'lambda IoUs: IoUs.mean()'
          IoUs:
            module: ignite.metrics
            class: IoU
            IoU:
              cm:
                module: ignite.metrics.confusion_matrix
                class: ConfusionMatrix
                ConfusionMatrix:
                  num_classes: 14
                  average: None
                  output_transform: 'lambda x: (x[0], x[1])'
                  device: '''cuda'''
              ignore_index: 0  # background
      mDice:
        module: ignite.metrics.metrics_lambda
        class: MetricsLambda
        MetricsLambda:
          f: 'lambda Dices: Dices.mean()'
          Dices:
            module: ignite.metrics
            class: DiceCoefficient
            DiceCoefficient:
              cm:
                module: ignite.metrics.confusion_matrix
                class: ConfusionMatrix
                ConfusionMatrix:
                  num_classes: 14
                  average: None
                  output_transform: 'lambda x: (x[0], x[1])'
                  device: '''cuda'''
              ignore_index: 0  # background
    attach_to:
      train_evaluator: '''train'''
      valid_evaluator: '''valid'''

screenlogger:
  module: flame.handlers.screenlogger
  class: ScreenLogger
  ScreenLogger:
    eval_names:
      - '''train''' 
      - '''valid'''

predictor:
  module: flame.handlers.field_predictor
  class: FieldPredictor
  FieldPredictor:
    evaluator_name: '''valid_evaluator'''
    output_dir: '''checkpoint/CCCD_front_chip/valid/'''
    image_extent: '''.jpg'''
    mask_extent: '''.png'''
    prob_threshold: 0.5
    mk_field_dir: True
    threshold: 0.00
    classes:
      BACKGROUND: [[0, 0, 0], 0, False, False, 0.]  # color, class_id, reduce_height, reduce_width, class_ratio
      HEADING: [[175, 153, 144], 1, True, False, 0.]
      V_ID: [[75, 25, 230], 2, True, False, 0.]
      # V_NAME1: [[128, 0, 0], 3, True, False, 0.]
      V_NAME2: [[48, 130, 245], 3, True, False, 0.]
      V_BD: [[128, 128, 0], 4, True, False, 0.]
      V_SEX: [[0, 101, 255], 5, True, False, 0.]
      V_NAT: [[100, 150, 255], 6, True, False, 0.]
      V_BP1: [[25, 225, 225], 7, True, False, 0.]
      V_BP2: [[75, 180, 60], 8, True, False, 0.]
      V_A1: [[180, 215, 255], 9, True, False, 0.]
      V_A2: [[240, 240, 70], 10, True, False, 0.]
      V_TL: [[100, 0, 100], 11, True, False, 0.]
      FIGURE: [[255, 190, 230], 12, False, False, 0.]
      QR_CODE: [[70, 70, 70], 13, False, False, 0.]
      QR: [[70, 70, 70], 13, False, False, 0.]
    output_transform: 'lambda x: (x[0], x[-1])'

writer:
  module: flame.handlers.tensorboard
  class: TensorBoard
  TensorBoard:
    logdir: '''checkpoint/CCCD_front_chip/'''

logger:
  module: flame.handlers.logger
  class: Logger
  Logger:
    logdir: '''checkpoint/CCCD_front_chip/'''
    logname: '''CCCD_front_chip'''

screenlogger:
  module: flame.handlers.screenlogger
  class: ScreenLogger
  ScreenLogger:
    classes:
      - '''HEADING'''
      - '''V_ID'''
      - '''V_NAME2'''
      - '''V_BD'''
      - '''V_SEX'''
      - '''V_NAT'''
      - '''V_BP1'''
      - '''V_BP2'''
      - '''V_A1'''
      - '''V_A2'''
      - '''V_TL'''
      - '''FIGURE'''
      - '''QR_CODE'''
    eval_names:
      - '''train'''
      - '''valid'''

history:
  module: flame.handlers.checkpoint
  class: History

checkpoint_loader:
  module: flame.handlers.checkpoint
  class: CheckpointLoader
  CheckpointLoader:
    checkpoint_path: ''''''
    mode: '''train'''

terminate_on_nan:
  module: flame.handlers.terminate_on_nan
  class: TerminateOnNan

lr_scheduler:
  module: flame.handlers.lr_scheduler
  class: LRScheduler
  LRScheduler:
    score_name: '''OHEM_ce_loss'''
    evaluator_name: '''valid_evaluator'''
    scheduler:
      module: flame.handlers.lr_schedulers
      class: WarmupPolyLR
      WarmupPolyLR:
        optimizer: config['optim']
        data_loader: config['data']['train']
        max_epochs: config['engine']['Trainer']['max_epochs']
        power: 0.9
        warmup_scale: 10
        warmup_ratio: 0.1
        warmup: '''exp'''
        last_epoch: -1

# early_stopping:
#   module: flame.handlers.early_stopping
#   class: EarlyStopping
#   EarlyStopping:
#     score_name: '''OHEM_ce_loss'''
#     evaluator_name: '''valid_evaluator'''
#     mode: '''min'''
#     patience: 20

best_saver:
  module: flame.handlers.checkpoint
  class: BestSaver
  BestSaver:
    dirname: '''checkpoint/CCCD_front_chip/'''
    score_name: '''OHEM_ce_loss'''
    evaluator_name: '''valid_evaluator'''
    mode: '''min'''
    n_saved: 1

backup_saver:
  module: flame.handlers.checkpoint
  class: BackupSaver
  BackupSaver:
    modules:
      - '''model'''
      - '''optim'''
      - '''backup_saver'''
      - '''best_saver'''
      - '''history'''
      - '''lr_scheduler'''
      # - '''early_stopping'''
    dirname: '''checkpoint/CCCD_front_chip/'''
    save_interval: 1
    n_saved: 1

engine:
  module: flame.core.engine.engine
  class: Trainer
  Trainer:
    dataset: config['data']['train']
    device: '''cuda'''
    max_epochs: 10000

extralibs:
  iaa: imgaug.augmenters
  torch: torch
